{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn autogluon tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "\n",
    "streets = [\"washington\", \"convention\", \"st_antoine\"]\n",
    "street = streets[0]\n",
    "\n",
    "targets = [\"Débit horaire\", \"Taux d'occupation\"]\n",
    "target_variable = targets[0]\n",
    "df = data_loader.load_data(street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether all the \"Date et heure de comptage\" are every 1 hour\n",
    "print(\"Checking if all the 'Date et heure de comptage' are every 1 hour.\")\n",
    "print(\"Start date:\", df['Date et heure de comptage'].min().tz_convert(\"Europe/Paris\"))\n",
    "print(\"End date:\", df['Date et heure de comptage'].max())\n",
    "# Generate a complete range of hourly timestamps between the min and max dates in the column\n",
    "full_range = pd.date_range(start=df['Date et heure de comptage'].min(), \n",
    "                           end=df['Date et heure de comptage'].max(), \n",
    "                           freq='h')\n",
    "\n",
    "# Find missing timestamps\n",
    "missing_dates = full_range.difference(df['Date et heure de comptage'])\n",
    "\n",
    "# Print results\n",
    "if missing_dates.empty:\n",
    "    print(\"No missing dates!\")\n",
    "else:\n",
    "    print(f\"Missing dates ({len(missing_dates)}):\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your original DataFrame with the datetime column 'Date et heure de comptage'\n",
    "\n",
    "# Step 1: Create a complete range of hourly timestamps\n",
    "full_range = pd.date_range(start=df['Date et heure de comptage'].min(), \n",
    "                           end=df['Date et heure de comptage'].max(), \n",
    "                           freq='h')\n",
    "\n",
    "# Step 2: Create a DataFrame with the full range and mark present/missing timestamps\n",
    "full_df = pd.DataFrame({'Date et heure de comptage': full_range})\n",
    "full_df['Data Present'] = full_df['Date et heure de comptage'].isin(df['Date et heure de comptage']).astype(int)\n",
    "\n",
    "# Set the datetime column as the index for plotting\n",
    "full_df.set_index('Date et heure de comptage', inplace=True)\n",
    "\n",
    "# Step 3: Plot the data availability over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "full_df['Data Present'].plot(drawstyle='steps-post', color='blue')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Data Present (1) or Missing (0)')\n",
    "plt.title('Data Availability Over Time')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows before reindexing:\", len(df))\n",
    "\n",
    "# end_date = df['Date et heure de comptage'].max()\n",
    "# Set end date to December 31st 2024\n",
    "end_date = pd.Timestamp('2024-12-31').tz_localize('UTC')\n",
    "\n",
    "# Step 1: Create a full hourly datetime range\n",
    "full_range = pd.date_range(start=df['Date et heure de comptage'].min(), \n",
    "                           end=end_date, \n",
    "                           freq='h')\n",
    "\n",
    "# Step 2: Reindex the DataFrame\n",
    "df = df.set_index('Date et heure de comptage')\n",
    "df = df.reindex(full_range)\n",
    "\n",
    "# Step 3: Rename the index to match the original column name\n",
    "df.index.name = 'Date et heure de comptage'\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "print(\"Number of rows after reindexing:\", len(df))\n",
    "\n",
    "# Step 4: Handle missing values (optional)\n",
    "# Option 1: Leave NaNs (explicitly missing data)\n",
    "# Option 2: Fill with a placeholder (e.g., 0)\n",
    "# df_reindexed.fillna(0, inplace=False)  # Or specify a method like \"ffill\" or \"bfill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the holiday and bank holiday columns\n",
    "\n",
    "# Load the holidays data\n",
    "holidays = pd.read_csv(\"data/holidays_bank_holidays.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Ensure the 'date' columns in both DataFrames are of the same type\n",
    "df['date'] = df[\"Date et heure de comptage\"].dt.date  # Convert to datetime.date\n",
    "holidays['date'] = holidays['date'].dt.date  # Convert to datetime.date\n",
    "\n",
    "# Merge the two DataFrames on the \"date\" column\n",
    "df = df.merge(holidays, on=\"date\", how=\"left\")\n",
    "\n",
    "# Fill missing values without using inplace=True\n",
    "df[\"vacances_zone_c\"] = df[\"vacances_zone_c\"].fillna(0)  # Assuming 0 means not a holiday\n",
    "df[\"ferie\"] = df[\"ferie\"].fillna(0)  # Assuming 0 means not a bank holiday\n",
    "\n",
    "# Drop the temporary \"date\" column if it's no longer needed\n",
    "df = df.drop(columns=[\"date\"])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime features from 'Date et heure de comptage'\n",
    "df['Year'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.year\n",
    "df['Month'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.month\n",
    "df['Day'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.day\n",
    "df['Hour'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.hour\n",
    "df['DayOfWeek'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.dayofweek\n",
    "new_features = ['Year', 'Month', 'Day', 'Hour', 'DayOfWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summaries of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3000:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#FF5733\", \"#33FF57\", \"#5733FF\", \"#FF33F9\"]\n",
    "for i in new_features:\n",
    "    plt.figure(figsize=(10,2),facecolor='w')\n",
    "    ax=sns.lineplot(x=df[i],y=\"Débit horaire\",data=df)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by DayOfWeek and Hour, and calculate the mean Débit horaire\n",
    "mean_values = df.groupby(['DayOfWeek', 'Hour'])['Débit horaire'].mean().reset_index()\n",
    "\n",
    "# Define day names\n",
    "days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "\n",
    "# Plot each day, offsetting the x-values by 24 hours per day\n",
    "for i, day_name in enumerate(days):\n",
    "    day_data = mean_values[mean_values['DayOfWeek'] == i]\n",
    "    # Shift the hours by 24*i to place each day side by side\n",
    "    shifted_hours = day_data['Hour'] + (i * 24)\n",
    "    sns.lineplot(x=shifted_hours, y=day_data['Débit horaire'], label=day_name)\n",
    "\n",
    "plt.xlabel(\"Hour (shifted by day)\")\n",
    "plt.ylabel(\"Mean Débit horaire\")\n",
    "plt.title(\"Mean Débit horaire by Hour for Each Day of the Week (Side-by-Side)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of 'Débit horaire'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Débit horaire'], kde=True, bins=30)\n",
    "plt.title('Distribution of Débit horaire')\n",
    "plt.xlabel('Débit horaire')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of 'Taux d\\'occupation'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Taux d\\'occupation'], kde=True, bins=30)\n",
    "plt.title('Distribution of Taux d\\'occupation')\n",
    "plt.xlabel('Taux d\\'occupation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of numerical features\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 'Débit horaire' over different hours of the day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df, x='Hour', y='Débit horaire', estimator='mean')\n",
    "plt.title('Average Débit horaire by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Débit horaire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 'Taux d\\'occupation' over different hours of the day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df, x='Hour', y='Taux d\\'occupation', estimator='mean')\n",
    "plt.title('Average Taux d\\'occupation by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Taux d\\'occupation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 'Débit horaire' by 'Etat trafic'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='Etat trafic', y='Débit horaire')\n",
    "plt.title('Average Débit horaire by Etat trafic')\n",
    "plt.xlabel('Etat trafic')\n",
    "plt.ylabel('Average Débit horaire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 'Taux d\\'occupation' by 'Etat trafic'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='Etat trafic', y='Taux d\\'occupation')\n",
    "plt.title('Average Taux d\\'occupation by Etat trafic')\n",
    "plt.xlabel('Etat trafic')\n",
    "plt.ylabel('Average Taux d\\'occupation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Chronos with Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Libelle', 'Identifiant arc', 'Identifiant noeud amont', 'Libelle noeud amont',\n",
    "                 'Identifiant noeud aval', 'Libelle noeud aval', 'Date debut dispo data', 'Date fin dispo data'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to one-hot encode\n",
    "# columns_to_encode = ['Etat trafic', 'Etat arc', 'nom_vacances', 'nom_jour_ferie', 'DayOfWeek', 'Month']\n",
    "# Apply one-hot encoding to the specified columns\n",
    "# df = pd.get_dummies(df, columns=columns_to_encode, prefix=columns_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TimeSeriesDataFrame\n",
    "# df[\"timestamp\"] = df[\"Date et heure de comptage\"]\n",
    "# df.set_index('Date et heure de comptage', inplace=True)\n",
    "\n",
    "# Add the required 'item_id' column (since this is a single time series, we use a constant ID)\n",
    "df['item_id'] = 'series_' + street\n",
    "\n",
    "df[\"Date et heure de comptage\"] = df[\"Date et heure de comptage\"]\n",
    "\n",
    "# Rename \"Date et heure de comptage\" to \"timestamp\"\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "df[\"timestamp\"] = df[\"Date et heure de comptage\"].dt.tz_localize(None)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# Rename \"Débit horaire\" to \"target\"\n",
    "\n",
    "df[\"target\"] = df[target_variable]\n",
    "# df[\"target\"] = df[\"Taux d'occupation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = TimeSeriesDataFrame.from_data_frame(\n",
    "    df,\n",
    "    id_column='item_id',\n",
    ")\n",
    "ts_df.infer_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define start and end times as pd.Timestamp\n",
    "start_date = pd.Timestamp(\"2024-11-01\")\n",
    "end_date = pd.Timestamp(\"2024-12-30\")\n",
    "\n",
    "# Filter data between start_date and end_date\n",
    "filtered_ts_df = ts_df.slice_by_time(start_time=start_date, end_time=end_date)\n",
    "\n",
    "# Plot the 'target' column\n",
    "for item_id in filtered_ts_df.item_ids:\n",
    "    group = filtered_ts_df.loc[item_id]\n",
    "    plt.plot(group.index, group['target'], label=f\"Item {item_id}\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Target Time Series (July 1st to October 31st)\")\n",
    "# plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside the plot\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff point for the test set\n",
    "cutoff_date = df[\"timestamp\"].max() - pd.DateOffset(months=2)\n",
    "# Set cutoff to 25 October 2024\n",
    "cutoff_date = pd.Timestamp(\"2024-10-01\")\n",
    "print(f\"Test set cutoff date: {cutoff_date}\")\n",
    "\n",
    "# Define the prediction length (e.g., 24 hours)\n",
    "prediction_length = 24 * 5\n",
    "\n",
    "# Split the data\n",
    "train_data = ts_df.loc[ts_df.index.get_level_values(\"timestamp\") <= cutoff_date]\n",
    "test_data = ts_df.loc[(ts_df.index.get_level_values(\"timestamp\") > cutoff_date) & (ts_df.index.get_level_values(\"timestamp\") <= cutoff_date + pd.DateOffset(days=10))]\n",
    "\n",
    "# train_data = ts_df.slice_by_time(pd.Timestamp(0), cutoff_date)\n",
    "# test_data = ts_df.slice_by_time(cutoff_date, pd.Timestamp(\"2024-12-31\"))\n",
    "\n",
    "# cutoff_index = ts_df.loc[ts_df.index.get_level_values(\"timestamp\") <= cutoff_date].shape[0]\n",
    "# train_data, test_data = ts_df.train_test_split(end_index=cutoff_index, prediction_length=prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "# Specify known covariates\n",
    "known_covariates_names = [\"Year\", \"Month\", \"Day\", \"Hour\", \"DayOfWeek\", \"vacances_zone_c\", \"ferie\"]\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=prediction_length,\n",
    "    verbosity=3,\n",
    "    freq='h',\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    # eval_metric=\"RMSE\"\n",
    ")\n",
    "\n",
    "# Train the predictor with fine-tuning\n",
    "predictor.fit(\n",
    "    train_data,\n",
    "    # presets='chronos_mini',\n",
    "    hyperparameters={\n",
    "        # \"Chronos\": {\n",
    "        #     \"use_mps\": True,\n",
    "        #     \"model_path\": \"amazon/chronos-bolt-tiny\",\n",
    "        #     \"fine_tune\": False\n",
    "        # }\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"amazon/chronos-bolt-mini\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            # {\"model_path\": \"amazon/chronos-bolt-tiny\", \"fine_tune\": True, \"ag_args\": {\"name_suffix\": \"FineTuned\"}},\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries.utils.forecast import get_forecast_horizon_index_ts_dataframe\n",
    "\n",
    "# Generate predictions\n",
    "# Without covariates\n",
    "# predictions = predictor.predict(train_data)\n",
    "\n",
    "# With covariates\n",
    "future_index = get_forecast_horizon_index_ts_dataframe(train_data, prediction_length=prediction_length)\n",
    "future_timestamps = future_index.get_level_values(\"timestamp\")\n",
    "known_covariates = test_data[known_covariates_names]  # Known covariates from test data\n",
    "predictions = predictor.predict(train_data, known_covariates=known_covariates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the evaluation dataset\n",
    "# Combine train_data and the first prediction_length rows of test_data\n",
    "eval_data = TimeSeriesDataFrame(pd.concat(\n",
    "    [\n",
    "        train_data,\n",
    "        test_data.groupby(\"item_id\").head(prediction_length)  # First prediction_length rows of test_data\n",
    "    ]\n",
    "))\n",
    "\n",
    "# Evaluate the predictor\n",
    "performance = predictor.evaluate(eval_data, metrics=[\"WQL\", \"RMSE\"])\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Evaluation Performance Metrics:\")\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cutoff_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure predictions and test data have the same structure\n",
    "# Assuming 'target' is the column in test_data and predictions\n",
    "\n",
    "# Plot predictions vs. actual values\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(test_data.index.get_level_values('timestamp'),\n",
    "         test_data['target'],\n",
    "         label='Actual Values',\n",
    "         linestyle='-')\n",
    "\n",
    "# Plot predicted values\n",
    "plt.plot(predictions.index.get_level_values('timestamp'),\n",
    "         predictions['mean'],\n",
    "         label='Predicted Values',\n",
    "         linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Target Value')\n",
    "plt.title('Predictions vs. Actual Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Align the MultiIndex between predictions and test data\n",
    "common_index = predictions.index.intersection(test_data.index)\n",
    "\n",
    "# Extract aligned actual values and predictions\n",
    "aligned_actuals = test_data.loc[common_index, \"target\"]\n",
    "aligned_predictions = predictions.loc[common_index, \"mean\"]\n",
    "\n",
    "# Drop NaN values\n",
    "valid_mask = ~aligned_actuals.isna() & ~aligned_predictions.isna()\n",
    "aligned_actuals = aligned_actuals[valid_mask]\n",
    "aligned_predictions = aligned_predictions[valid_mask]\n",
    "\n",
    "# Ensure indices are aligned for plotting\n",
    "aligned_actuals = aligned_actuals.sort_index()\n",
    "aligned_predictions = aligned_predictions.sort_index()\n",
    "\n",
    "# Calculate RMSE for train_data predictions\n",
    "rmse_train = np.sqrt(mean_squared_error(aligned_actuals, aligned_predictions))\n",
    "print(f\"RMSE (Next 5 days after train): {rmse_train}\")\n",
    "\n",
    "# Evaluate RMSE with extended train_data + partial test_data\n",
    "durations = [24, 48, 72, 96]  # Example durations in hours\n",
    "rmse_list = []\n",
    "\n",
    "for duration in durations:\n",
    "    # Extend train_data with the first 'duration' hours of test_data\n",
    "    extended_train_data = pd.concat([\n",
    "        train_data,\n",
    "        test_data.iloc[:duration]\n",
    "    ])\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions_extended = predictor.predict(extended_train_data, known_covariates=known_covariates)\n",
    "    \n",
    "    # Align predictions and actual values\n",
    "    extended_common_index = predictions_extended.index.intersection(test_data.index)\n",
    "    aligned_actuals = test_data.loc[extended_common_index, \"target\"]\n",
    "    aligned_predictions = predictions_extended.loc[extended_common_index, \"mean\"]\n",
    "    \n",
    "    # Drop NaN values\n",
    "    valid_mask = ~aligned_actuals.isna() & ~aligned_predictions.isna()\n",
    "    aligned_actuals = aligned_actuals[valid_mask]\n",
    "    aligned_predictions = aligned_predictions[valid_mask]\n",
    "    \n",
    "    # Ensure indices are aligned for RMSE calculation\n",
    "    aligned_actuals = aligned_actuals.sort_index()\n",
    "    aligned_predictions = aligned_predictions.sort_index()\n",
    "    \n",
    "    # Calculate RMSE for the current duration\n",
    "    rmse = np.sqrt(mean_squared_error(aligned_actuals, aligned_predictions))\n",
    "    rmse_list.append(rmse)\n",
    "    print(f\"RMSE after adding {duration} hours of test data: {rmse}\")\n",
    "\n",
    "# Calculate average RMSE over all durations\n",
    "average_rmse = np.mean(rmse_list)\n",
    "print(f\"Average RMSE: {average_rmse}\")\n",
    "\n",
    "# Plot predictions vs actual values for train_data\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(\n",
    "    aligned_actuals.index.get_level_values('timestamp'),\n",
    "    aligned_actuals.values,\n",
    "    label='Actual Values',\n",
    "    linestyle='-'\n",
    ")\n",
    "\n",
    "# Plot predicted values for train_data\n",
    "plt.plot(\n",
    "    aligned_predictions.index.get_level_values('timestamp'),\n",
    "    aligned_predictions.values,\n",
    "    label='Predicted Values (Train Data)',\n",
    "    linestyle='--'\n",
    ")\n",
    "\n",
    "# Optionally, add extended train_data predictions to the plot\n",
    "for duration in durations:\n",
    "    extended_train_data = pd.concat([train_data, test_data.iloc[:duration]])\n",
    "    predictions_extended = predictor.predict(extended_train_data, known_covariates=known_covariates)\n",
    "    extended_common_index = predictions_extended.index.intersection(test_data.index)\n",
    "    aligned_predictions = predictions_extended.loc[extended_common_index, \"mean\"]\n",
    "    \n",
    "    # Ensure indices are aligned for plotting\n",
    "    aligned_predictions = aligned_predictions.sort_index()\n",
    "    \n",
    "    plt.plot(\n",
    "        aligned_predictions.index.get_level_values('timestamp'),\n",
    "        aligned_predictions.values,\n",
    "        linestyle=':',\n",
    "        label=f'Predicted (Train + {duration}h Test)'\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Target Value')\n",
    "plt.title('Predictions vs. Actual Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = TimeSeriesDataFrame.from_data_frame(\n",
    "    df,\n",
    "    id_column='item_id',\n",
    ")\n",
    "ts_df.infer_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff date to 6 December 2024 00:30:00 Paris time\n",
    "cutoff_date = pd.Timestamp(\"2024-12-06 00:30:00\").tz_localize(\"Europe/Paris\").tz_convert(\"UTC\").tz_localize(None)\n",
    "print(f\"Predictions cutoff date: {cutoff_date.tz_localize('UTC').tz_convert('Europe/Paris')}\")\n",
    "\n",
    "# Define the prediction length (e.g., 24 hours)\n",
    "prediction_length = 24 * 5\n",
    "\n",
    "# Split the data\n",
    "train_data = ts_df.loc[ts_df.index.get_level_values(\"timestamp\") <= cutoff_date]\n",
    "test_data = ts_df.loc[(ts_df.index.get_level_values(\"timestamp\") > cutoff_date)]\n",
    "\n",
    "# Print last date of train_data\n",
    "print(f\"Last date of train_data: {train_data.index.get_level_values('timestamp').max().tz_localize('UTC').tz_convert('Europe/Paris')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the predictor without fine-tuning\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=prediction_length,\n",
    "    verbosity=3,\n",
    "    freq='h',\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    # eval_metric=\"RMSE\"\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_data,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"amazon/chronos-bolt-mini\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            # {\"model_path\": \"amazon/chronos-bolt-tiny\", \"fine_tune\": True, \"ag_args\": {\"name_suffix\": \"FineTuned\"}},\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries.utils.forecast import get_forecast_horizon_index_ts_dataframe\n",
    "\n",
    "# Generate predictions\n",
    "# Without covariates\n",
    "# predictions = predictor.predict(train_data)\n",
    "\n",
    "# With covariates\n",
    "future_index = get_forecast_horizon_index_ts_dataframe(train_data, prediction_length=prediction_length)\n",
    "future_timestamps = future_index.get_level_values(\"timestamp\")\n",
    "known_covariates = test_data[known_covariates_names]  # Known covariates from test data\n",
    "predictions = predictor.predict(train_data, known_covariates=known_covariates)\n",
    "\n",
    "\n",
    "print(\"First prediction date:\", predictions.index.get_level_values(\"timestamp\").min().tz_localize(\"UTC\").tz_convert(\"Europe/Paris\"))\n",
    "print(\"Last prediction date:\", predictions.index.get_level_values(\"timestamp\").max().tz_localize(\"UTC\").tz_convert(\"Europe/Paris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot predictions vs. actual values\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot predicted values\n",
    "plt.plot(predictions.index.get_level_values('timestamp'),\n",
    "         predictions['mean'],\n",
    "         label='Predicted Values',\n",
    "         linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Target Value')\n",
    "plt.title('Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "output_dir = \"predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert the TimeSeriesDataFrame to a pandas DataFrame\n",
    "predictions_df = predictions.to_data_frame()\n",
    "\n",
    "# Iterate over each time series in the DataFrame\n",
    "for street, street_data in predictions_df.groupby(level=0):  # Assumes street info is in the first level of the index\n",
    "    # Save to a CSV file\n",
    "    output_file = f\"{output_dir}/{street}-{target_variable}.csv\"\n",
    "    street_data.reset_index(level=0, drop=True).to_csv(output_file)\n",
    "\n",
    "print(f\"Predictions saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
