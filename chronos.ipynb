{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Prediction for Optimized Delivery Scheduling\n",
    "\n",
    "This notebook focuses on predicting traffic metrics, such as **hourly vehicle flow (Débit horaire)** and **lane occupancy rate (Taux d'occupation)**, to help optimize delivery operations for the company LivraisonCo. \n",
    "\n",
    "The key objectives are:\n",
    "- To preprocess and clean traffic data collected from sensors in Paris streets.\n",
    "- To analyze data patterns and visualize traffic trends.\n",
    "- To train and validate predictive models for forecasting traffic conditions on selected road segments.\n",
    "- To evaluate the model performance using metrics like RMSE and prepare predictions for the specified target period.\n",
    "\n",
    "We use **AutoGluon’s Chronos** model as the primary predictive tool due to its robust handling of time-series data. Additional tools include Python libraries for visualization and exploratory data analysis (EDA).\n",
    "\n",
    "This notebook is divided into the following sections:\n",
    "1. **Data Loading and Preprocessing**: Loading raw traffic data and preparing it for analysis.\n",
    "2. **Exploratory Data Analysis (EDA)**: Analyzing and visualizing traffic patterns to extract insights.\n",
    "3. **Feature Engineering**: Creating additional features, such as holidays and time-based variables, to improve model predictions.\n",
    "4. **Traffic Prediction Model**: Training the Chronos model, generating predictions, and evaluating performance.\n",
    "5. **Results and Visualization**: Comparing predicted and actual traffic values and saving the final predictions.\n",
    "\n",
    "Each cell is accompanied by markdown annotations to explain its purpose in the overall workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn autogluon tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "\n",
    "In this cell, we import all the libraries required for data preprocessing, visualization, and modeling:\n",
    "- **pandas**: For data manipulation and analysis.\n",
    "- **numpy**: For numerical operations.\n",
    "- **matplotlib & seaborn**: For data visualization, including line plots, histograms, and heatmaps.\n",
    "- **autogluon.timeseries**: To create and manage time-series data structures and for training the Chronos prediction model.\n",
    "- **os**: For interacting with the file system (e.g., saving results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Traffic Data for Selected Street\n",
    "\n",
    "### Purpose:\n",
    "This cell:\n",
    "1. **Initializes the data loading process** using a helper function `load_data` from `data_loader.py`.\n",
    "2. Selects one of the predefined streets (`washington`, `convention`, `st_antoine`) as the data source for analysis.\n",
    "3. Specifies the target variable for prediction (`Débit horaire` or `Taux d'occupation`).\n",
    "\n",
    "### Key Components:\n",
    "- **`streets` list:** Defines the available street options for data loading.\n",
    "- **`target_variable`:** Specifies the primary traffic metric to analyze.\n",
    "- **`load_data` function:** Reads and preprocesses the traffic data for the selected street, ensuring:\n",
    "  - Data is filtered for the relevant street and Arc ID.\n",
    "  - Dates are converted to UTC format for consistency.\n",
    "  - Duplicates and invalid entries are removed.\n",
    "\n",
    "The resulting DataFrame (`df`) will serve as the input for further preprocessing and modeling steps.\n",
    "\n",
    "### Notes:\n",
    "- The `data_loader.py` script dynamically handles column renaming, timezone adjustments, and concatenation of multiple data files.\n",
    "- For the selected street (`washington` by default), the data is pre-filtered to include only rows with the corresponding Arc ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader\n",
    "\n",
    "streets = [\"washington\", \"convention\", \"st_antoine\"]\n",
    "street = streets[0]\n",
    "\n",
    "targets = [\"Débit horaire\", \"Taux d'occupation\"]\n",
    "target_variable = targets[0]\n",
    "df = data_loader.load_data(street)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Pandas Display Options\n",
    "This cell ensures that all columns in the DataFrame are displayed when printed. \n",
    "This is particularly useful for inspecting large datasets with many columns, such as traffic data in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview and Summary\n",
    "This cell displays the basic structure of the loaded DataFrame, including:\n",
    "- **Column names**.\n",
    "- **Data types** (e.g., integers, floats, datetime, etc.).\n",
    "- **Non-null counts** for each column to identify missing data.\n",
    "- **Memory usage** for understanding the dataset's size.\n",
    "\n",
    "Key Points:\n",
    "- Look for columns with missing data (e.g., `Débit horaire`, `Taux d'occupation`).\n",
    "- Verify that the `Date et heure de comptage` column is properly parsed as a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Continuity for 'Date et heure de comptage'\n",
    "This cell verifies if the `Date et heure de comptage` column contains hourly timestamps without gaps. \n",
    "\n",
    "Steps:\n",
    "1. **Generate a complete hourly range** from the minimum to maximum timestamps in the data.\n",
    "2. **Compare the range** with the existing timestamps in the dataset to identify missing entries.\n",
    "3. **Print results**:\n",
    "   - Start and end dates of the data.\n",
    "   - List of missing timestamps (if any).\n",
    "\n",
    "Key Points:\n",
    "- Identifying missing data is crucial for time-series forecasting.\n",
    "- This information will guide how to handle gaps in the data during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wether all the \"Date et heure de comptage\" are every 1 hour\n",
    "print(\"Checking if all the 'Date et heure de comptage' are every 1 hour.\")\n",
    "print(\"Start date:\", df['Date et heure de comptage'].min().tz_convert(\"Europe/Paris\"))\n",
    "print(\"End date:\", df['Date et heure de comptage'].max())\n",
    "# Generate a complete range of hourly timestamps between the min and max dates in the column\n",
    "full_range = pd.date_range(start=df['Date et heure de comptage'].min(), \n",
    "                           end=df['Date et heure de comptage'].max(), \n",
    "                           freq='h')\n",
    "\n",
    "# Find missing timestamps\n",
    "missing_dates = full_range.difference(df['Date et heure de comptage'])\n",
    "\n",
    "# Print results\n",
    "if missing_dates.empty:\n",
    "    print(\"No missing dates!\")\n",
    "else:\n",
    "    print(f\"Missing dates ({len(missing_dates)}):\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data Availability Over Time\n",
    "This cell generates a visualization of data availability, highlighting any gaps in the time-series data.\n",
    "\n",
    "Steps:\n",
    "1. Create a DataFrame representing the complete range of hourly timestamps.\n",
    "2. Mark whether each timestamp is present (1) or missing (0).\n",
    "3. Plot the results as a step chart to show the presence or absence of data over time.\n",
    "\n",
    "Key Insights:\n",
    "- This visualization helps identify patterns of missing data, such as periods of missing records or irregular sampling.\n",
    "- It provides a clear overview of the dataset's completeness for further analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a complete range of hourly timestamps\n",
    "full_range = pd.date_range(start=df['Date et heure de comptage'].min(), \n",
    "                           end=df['Date et heure de comptage'].max(), \n",
    "                           freq='h')\n",
    "\n",
    "# Step 2: Create a DataFrame with the full range and mark present/missing timestamps\n",
    "full_df = pd.DataFrame({'Date et heure de comptage': full_range})\n",
    "full_df['Data Present'] = full_df['Date et heure de comptage'].isin(df['Date et heure de comptage']).astype(int)\n",
    "\n",
    "# Set the datetime column as the index for plotting\n",
    "full_df.set_index('Date et heure de comptage', inplace=True)\n",
    "\n",
    "# Step 3: Plot the data availability over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "full_df['Data Present'].plot(drawstyle='steps-post', color='blue')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Data Present (1) or Missing (0)')\n",
    "plt.title('Data Availability Over Time')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindex DataFrame to Ensure Hourly Continuity\n",
    "\n",
    "This cell ensures that the dataset contains a complete range of hourly timestamps, even for missing hours.\n",
    "\n",
    "### Steps:\n",
    "1. **Generate a Full Hourly Range:**\n",
    "   - From the earliest timestamp in the dataset (`Date et heure de comptage`) to the fixed end date (December 31st, 2024).\n",
    "   - The full range ensures consistency in the time-series data.\n",
    "\n",
    "2. **Reindex the DataFrame:**\n",
    "   - Set `Date et heure de comptage` as the index.\n",
    "   - Align the dataset with the full range of hourly timestamps, inserting rows with missing timestamps.\n",
    "\n",
    "3. **Handle Missing Values:**\n",
    "   - Missing values for new rows (timestamps) are left as `NaN` for now.\n",
    "   - Placeholder values (e.g., `0` or forward fill/backward fill) can be applied based on the modeling needs.\n",
    "\n",
    "### Key Insights:\n",
    "- The difference in row counts before and after reindexing indicates the number of missing timestamps filled with `NaN`.\n",
    "- This step standardizes the time index, which is critical for time-series forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows before reindexing:\", len(df))\n",
    "\n",
    "# end_date = df['Date et heure de comptage'].max()\n",
    "# Set end date to December 31st 2024\n",
    "end_date = pd.Timestamp('2024-12-31').tz_localize('UTC')\n",
    "\n",
    "# Step 1: Create a full hourly datetime range\n",
    "full_range = pd.date_range(start=df['Date et heure de comptage'].min(), \n",
    "                           end=end_date, \n",
    "                           freq='h')\n",
    "\n",
    "# Step 2: Reindex the DataFrame\n",
    "df = df.set_index('Date et heure de comptage')\n",
    "df = df.reindex(full_range)\n",
    "\n",
    "# Step 3: Rename the index to match the original column name\n",
    "df.index.name = 'Date et heure de comptage'\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "print(\"Number of rows after reindexing:\", len(df))\n",
    "\n",
    "# Step 4: Handle missing values (optional)\n",
    "# Option 1: Leave NaNs (explicitly missing data)\n",
    "# Option 2: Fill with a placeholder (e.g., 0)\n",
    "# df_reindexed.fillna(0, inplace=False)  # Or specify a method like \"ffill\" or \"bfill\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Holiday and Bank Holiday Information\n",
    "\n",
    "### Purpose:\n",
    "This cell enriches the dataset with holiday-related information, which can be important predictors for traffic patterns.\n",
    "\n",
    "### Steps:\n",
    "1. **Load Holiday Data:**\n",
    "   - Reads a CSV file containing holiday and bank holiday information.\n",
    "   - The `date` column in the holiday data is parsed as a datetime object for consistency.\n",
    "\n",
    "2. **Merge with Traffic Data:**\n",
    "   - Converts the `Date et heure de comptage` column from the traffic data to a `date` format.\n",
    "   - Merges the holiday data with the traffic dataset on the `date` column.\n",
    "   - Columns `vacances_zone_c` (school holidays) and `ferie` (bank holidays) are added to the traffic data.\n",
    "\n",
    "3. **Handle Missing Values:**\n",
    "   - Missing values in `vacances_zone_c` and `ferie` are filled with `0`, indicating no holiday.\n",
    "\n",
    "4. **Clean Temporary Columns:**\n",
    "   - Drops the temporary `date` column after merging to maintain a clean DataFrame.\n",
    "\n",
    "### Key Insights:\n",
    "- Adding holiday and bank holiday information enhances the dataset by including features that impact traffic behavior.\n",
    "- The enriched dataset now contains additional columns such as:\n",
    "  - **`vacances_zone_c`**: Indicates if it is a school holiday.\n",
    "  - **`ferie`**: Indicates if it is a bank holiday.\n",
    "  - **`nom_vacances` and `nom_jour_ferie`**: Names of the specific holiday (optional but useful for analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the holiday and bank holiday columns\n",
    "\n",
    "# Load the holidays data\n",
    "holidays = pd.read_csv(\"data/holidays_bank_holidays.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Ensure the 'date' columns in both DataFrames are of the same type\n",
    "df['date'] = df[\"Date et heure de comptage\"].dt.date  # Convert to datetime.date\n",
    "holidays['date'] = holidays['date'].dt.date  # Convert to datetime.date\n",
    "\n",
    "# Merge the two DataFrames on the \"date\" column\n",
    "df = df.merge(holidays, on=\"date\", how=\"left\")\n",
    "\n",
    "# Fill missing values without using inplace=True\n",
    "df[\"vacances_zone_c\"] = df[\"vacances_zone_c\"].fillna(0)  # Assuming 0 means not a holiday\n",
    "df[\"ferie\"] = df[\"ferie\"].fillna(0)  # Assuming 0 means not a bank holiday\n",
    "\n",
    "# Drop the temporary \"date\" column if it's no longer needed\n",
    "df = df.drop(columns=[\"date\"])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Missing Values in the Dataset\n",
    "\n",
    "### Purpose:\n",
    "This cell identifies the extent of missing values in each column of the dataset, which helps in planning the preprocessing and data cleaning steps.\n",
    "\n",
    "### Key Insights:\n",
    "- **Critical Columns:** \n",
    "  - Columns like `Débit horaire` and `Taux d'occupation` (primary targets for modeling) have some missing values that need to be addressed.\n",
    "- **Less Critical Columns:** \n",
    "  - Columns like `Date debut dispo data` and `Date fin dispo data` have a high number of missing values, and their relevance should be evaluated.\n",
    "- **Holiday Columns:**\n",
    "  - Columns such as `vacances_zone_c` and `ferie` are fully populated, ensuring reliable feature information.\n",
    "\n",
    "### Next Steps:\n",
    "- Decide how to handle missing values:\n",
    "  - For target variables like `Débit horaire` and `Taux d'occupation`, filling techniques such as interpolation or forward fill/backward fill may be applied.\n",
    "  - Drop columns with excessive missing data (e.g., `Date debut dispo data`).\n",
    "  - For categorical columns (e.g., `Etat trafic`), consider filling with the most frequent value or a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract DateTime Features for Model Enrichment\n",
    "\n",
    "### Purpose:\n",
    "This cell extracts additional temporal features from the `Date et heure de comptage` column to enhance the dataset for time-series modeling.\n",
    "\n",
    "### Features Extracted:\n",
    "1. **Year**: Captures the annual trend or seasonality.\n",
    "2. **Month**: Helps model monthly patterns (e.g., increased traffic in December).\n",
    "3. **Day**: Identifies daily variations.\n",
    "4. **Hour**: Crucial for capturing hourly traffic patterns.\n",
    "5. **DayOfWeek**: Encodes the day of the week (0 = Monday, 6 = Sunday), which is useful for modeling weekday vs. weekend effects.\n",
    "\n",
    "### Why These Features Matter:\n",
    "- **Temporal Features:** Time-based features are critical for capturing periodic behaviors in traffic data.\n",
    "- **Modeling Utility:** These features can be used directly in machine learning models or for aggregating and analyzing data patterns.\n",
    "\n",
    "### Next Steps:\n",
    "- Explore the extracted features to identify trends and validate their utility in modeling.\n",
    "- Incorporate these features as inputs for the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime features from 'Date et heure de comptage'\n",
    "df['Year'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.year\n",
    "df['Month'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.month\n",
    "df['Day'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.day\n",
    "df['Hour'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.hour\n",
    "df['DayOfWeek'] = df['Date et heure de comptage'].dt.tz_convert(\"Europe/Paris\").dt.dayofweek\n",
    "new_features = ['Year', 'Month', 'Day', 'Hour', 'DayOfWeek']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Summary of Numerical Columns\n",
    "\n",
    "### Overview:\n",
    "This step provides a statistical summary of all numerical columns in the dataset, which is essential for understanding the data's distribution and range.\n",
    "\n",
    "### Key Metrics:\n",
    "- **Count:** Number of non-missing values in each column.\n",
    "- **Mean & Std:** Average and standard deviation, highlighting central tendency and variability.\n",
    "- **Min & Max:** Extremes of the dataset, useful for identifying outliers.\n",
    "- **25%, 50%, 75%:** Percentile values (quartiles), showing the spread of the data.\n",
    "\n",
    "### Insights from the Output:\n",
    "- **Débit horaire (Hourly Traffic Flow):**\n",
    "  - Mean traffic flow is ~1185 vehicles/hour, with a wide range from 0 to over 1600 (75th percentile).\n",
    "- **Taux d'occupation (Occupancy Rate):**\n",
    "  - Mean occupancy is ~11.8%, with high variability (standard deviation ~9.8%).\n",
    "- **Temporal Features:**\n",
    "  - `Year` spans from 2010 to 2024, with uniform distribution across months, days, and hours.\n",
    "- **Holiday Features:**\n",
    "  - Binary features like `vacances_zone_c` and `ferie` indicate that ~33% of the data corresponds to holidays or vacations.\n",
    "\n",
    "### Next Steps:\n",
    "- Use this information to identify potential outliers or anomalies in critical columns like `Débit horaire` and `Taux d'occupation`.\n",
    "- Inform feature engineering and data preprocessing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical summaries of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3000:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Trends in `Débit horaire` Across Temporal Features\n",
    "\n",
    "### Purpose:\n",
    "This section visualizes the variations in hourly traffic flow (`Débit horaire`) across key temporal features:\n",
    "1. **Yearly Trend:** Highlights changes in traffic flow over the years.\n",
    "2. **Monthly Seasonality:** Displays monthly variations, which may reveal peak and off-peak months.\n",
    "3. **Daily Patterns:** Examines fluctuations in traffic on different days of the month.\n",
    "4. **Hourly Patterns:** Shows typical daily traffic cycles.\n",
    "5. **Weekly Trends:** Explores variations across days of the week (e.g., weekdays vs weekends).\n",
    "\n",
    "### Observations:\n",
    "- **Year:** Traffic flow decreases over time, with some fluctuations.\n",
    "- **Month:** Peak traffic is observed mid-year, with dips in certain months (e.g., August).\n",
    "- **Day:** Traffic varies across days, showing potential for weekday-weekend effects.\n",
    "- **Hour:** Clear daily cycle, with peaks during rush hours and drops at night.\n",
    "- **Day of Week:** Traffic increases during the weekdays and drops sharply on weekends.\n",
    "\n",
    "### Next Steps:\n",
    "- Use these visualizations to identify critical patterns and validate their significance for model training.\n",
    "- Consider encoding these temporal features explicitly in the machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#FF5733\", \"#33FF57\", \"#5733FF\", \"#FF33F9\"]\n",
    "for i in new_features:\n",
    "    plt.figure(figsize=(10,2),facecolor='w')\n",
    "    ax=sns.lineplot(x=df[i],y=\"Débit horaire\",data=df)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Hourly Traffic by Day of the Week (Side-by-Side Visualization)\n",
    "\n",
    "### Objective:\n",
    "This plot provides a detailed view of the average hourly traffic (`Débit horaire`) for each day of the week. By shifting the x-axis for each day, we can compare daily traffic patterns side-by-side.\n",
    "\n",
    "### Key Features:\n",
    "1. **Grouping by Day and Hour:**\n",
    "   - The dataset is grouped by `DayOfWeek` and `Hour` to calculate the mean `Débit horaire` for each combination.\n",
    "2. **Visual Separation by Day:**\n",
    "   - The x-axis is adjusted to place each day's data sequentially, making it easier to compare trends.\n",
    "\n",
    "### Observations:\n",
    "- **Weekdays (Monday to Friday):**\n",
    "  - Traffic is high during typical morning (7-10 AM) and evening (5-8 PM) rush hours.\n",
    "- **Weekends (Saturday and Sunday):**\n",
    "  - Traffic is more evenly distributed, with lower peaks compared to weekdays.\n",
    "- **Friday:**\n",
    "  - Displays a distinct pattern, combining weekday rush hour trends with increasing evening traffic.\n",
    "\n",
    "### Importance for Modeling:\n",
    "- Highlights the necessity of incorporating both `DayOfWeek` and `Hour` features into predictive models.\n",
    "- Demonstrates potential differences in traffic behavior across weekdays and weekends, which can significantly influence predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by DayOfWeek and Hour, and calculate the mean Débit horaire\n",
    "mean_values = df.groupby(['DayOfWeek', 'Hour'])['Débit horaire'].mean().reset_index()\n",
    "\n",
    "# Define day names\n",
    "days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "\n",
    "# Plot each day, offsetting the x-values by 24 hours per day\n",
    "for i, day_name in enumerate(days):\n",
    "    day_data = mean_values[mean_values['DayOfWeek'] == i]\n",
    "    # Shift the hours by 24*i to place each day side by side\n",
    "    shifted_hours = day_data['Hour'] + (i * 24)\n",
    "    sns.lineplot(x=shifted_hours, y=day_data['Débit horaire'], label=day_name)\n",
    "\n",
    "plt.xlabel(\"Hour (shifted by day)\")\n",
    "plt.ylabel(\"Mean Débit horaire\")\n",
    "plt.title(\"Mean Débit horaire by Hour for Each Day of the Week (Side-by-Side)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Analysis: `Débit horaire` and `Taux d'occupation`\n",
    "\n",
    "### Objective:\n",
    "These visualizations explore the distribution of the two primary variables:\n",
    "1. **`Débit horaire` (Hourly Traffic Flow):** Measures the number of vehicles per hour.\n",
    "2. **`Taux d'occupation` (Lane Occupancy Rate):** Indicates the percentage of lane usage.\n",
    "\n",
    "### Key Observations:\n",
    "1. **`Débit horaire`:**\n",
    "   - The distribution is slightly **right-skewed**, with most values between 500 and 2000.\n",
    "   - Peaks around ~1000 suggest typical traffic conditions, while higher values may represent traffic jams or high-demand periods.\n",
    "2. **`Taux d'occupation`:**\n",
    "   - Highly **right-skewed**, with the majority of values below 20%.\n",
    "   - Very few observations exceed 50%, which may represent heavily congested scenarios.\n",
    "\n",
    "### Insights for Modeling:\n",
    "- Both distributions exhibit skewness, which might require transformations (e.g., log scale) to normalize for certain machine learning models.\n",
    "- The observed patterns confirm significant variability in traffic and occupancy, underlining the importance of feature engineering and robust modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of 'Débit horaire'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Débit horaire'], kde=True, bins=30)\n",
    "plt.title('Distribution of Débit horaire')\n",
    "plt.xlabel('Débit horaire')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of 'Taux d\\'occupation'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Taux d\\'occupation'], kde=True, bins=30)\n",
    "plt.title('Distribution of Taux d\\'occupation')\n",
    "plt.xlabel('Taux d\\'occupation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix of Numerical Features\n",
    "\n",
    "### Objective:\n",
    "This heatmap visualizes the correlations between numerical features in the dataset. Correlation values range from -1 to 1:\n",
    "- **Positive Correlation:** A value closer to 1 indicates that as one feature increases, the other tends to increase.\n",
    "- **Negative Correlation:** A value closer to -1 indicates an inverse relationship.\n",
    "- **No Correlation:** A value near 0 implies little to no linear relationship.\n",
    "\n",
    "### Key Observations:\n",
    "1. **`Débit horaire` (Hourly Traffic Flow):**\n",
    "   - Moderately correlated with `Hour` (0.41), reflecting daily traffic patterns.\n",
    "   - Weak correlation with `Taux d'occupation` (0.26), suggesting these metrics have distinct behaviors.\n",
    "2. **`Taux d'occupation` (Lane Occupancy Rate):**\n",
    "   - Similar weak correlation with `Hour` (0.31).\n",
    "3. **Holidays (`vacances_zone_c`, `ferie`):**\n",
    "   - Weak or no correlation with traffic metrics, indicating that holidays might not directly impact traffic in this dataset.\n",
    "4. **Temporal Features:**\n",
    "   - `Year` shows a negative correlation with `Débit horaire` (-0.47), possibly reflecting a decline in traffic over time.\n",
    "\n",
    "### Importance for Analysis:\n",
    "- Features like `Hour` and `Year` appear to have the strongest relationships with traffic flow.\n",
    "- Weak correlations suggest that additional derived features or non-linear relationships may be necessary to improve predictions.\n",
    "- The matrix aids in identifying redundant features (e.g., `vacances_zone_a`, `vacances_zone_b`, `vacances_zone_c` are highly correlated) for potential dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of numerical features\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Trends in Traffic Flow and Lane Occupancy\n",
    "\n",
    "### Purpose:\n",
    "These plots highlight the hourly patterns of:\n",
    "1. **`Débit horaire` (Hourly Traffic Flow):** The average number of vehicles per hour.\n",
    "2. **`Taux d'occupation` (Lane Occupancy Rate):** The average percentage of lane usage per hour.\n",
    "\n",
    "### Observations:\n",
    "1. **`Débit horaire`:**\n",
    "   - Traffic starts increasing early in the morning, peaking around 7-10 AM (morning rush hour).\n",
    "   - A second peak is observed in the evening around 4-7 PM (evening rush hour).\n",
    "   - Traffic flow decreases significantly during the night (12-5 AM).\n",
    "\n",
    "2. **`Taux d'occupation`:**\n",
    "   - The occupancy rate follows a similar trend, with peaks during rush hours.\n",
    "   - Lower occupancy during off-peak hours (night and early morning) suggests reduced congestion.\n",
    "\n",
    "### Insights for Modeling:\n",
    "- These patterns confirm the importance of the `Hour` feature in predictive models.\n",
    "- Rush hour effects and cyclical trends can be modeled explicitly using time-series techniques or engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 'Débit horaire' over different hours of the day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df, x='Hour', y='Débit horaire', estimator='mean')\n",
    "plt.title('Average Débit horaire by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Débit horaire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 'Taux d\\'occupation' over different hours of the day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df, x='Hour', y='Taux d\\'occupation', estimator='mean')\n",
    "plt.title('Average Taux d\\'occupation by Hour')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Taux d\\'occupation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of `Débit horaire` and `Taux d'occupation` by `Etat trafic`\n",
    "\n",
    "### Objective:\n",
    "These bar plots illustrate how traffic flow (`Débit horaire`) and lane occupancy (`Taux d'occupation`) vary across different traffic states (`Etat trafic`).\n",
    "\n",
    "### Observations:\n",
    "1. **`Débit horaire`:**\n",
    "   - Traffic flow is highest during certain states (e.g., `2` and `3`), likely representing free-flowing or moderately congested traffic.\n",
    "   - Lower flow occurs in `Bloqué` (Blocked) or `Inconnu` (Unknown) states, suggesting significant congestion or data anomalies.\n",
    "\n",
    "2. **`Taux d'occupation`:**\n",
    "   - Lane occupancy peaks in states labeled as `4` and `Bloqué`, which likely correspond to heavy congestion.\n",
    "   - Low occupancy during free-flowing states (e.g., `Fluide`) indicates smoother traffic.\n",
    "\n",
    "### Note:\n",
    "- **Data Issue:** The `Etat trafic` categories have not been correctly renamed by the loader script, making interpretation harder. Resolving this will provide clearer insights into traffic states.\n",
    "\n",
    "### Next Steps:\n",
    "1. Fix the categorical renaming issue for `Etat trafic`.\n",
    "2. Re-analyze to confirm these patterns.\n",
    "3. Consider the impact of these states in predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 'Débit horaire' by 'Etat trafic'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='Etat trafic', y='Débit horaire')\n",
    "plt.title('Average Débit horaire by Etat trafic')\n",
    "plt.xlabel('Etat trafic')\n",
    "plt.ylabel('Average Débit horaire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 'Taux d\\'occupation' by 'Etat trafic'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df, x='Etat trafic', y='Taux d\\'occupation')\n",
    "plt.title('Average Taux d\\'occupation by Etat trafic')\n",
    "plt.xlabel('Etat trafic')\n",
    "plt.ylabel('Average Taux d\\'occupation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Using Chronos and AutoGluon\n",
    "\n",
    "### Objective:\n",
    "This section focuses on leveraging **Chronos** and **AutoGluon** for time-series forecasting of traffic metrics (`Débit horaire` and `Taux d'occupation`). The goals include:\n",
    "1. Predicting future traffic flow and lane occupancy over a defined prediction horizon.\n",
    "2. Evaluating model performance using metrics like RMSE and WQL.\n",
    "3. Utilizing known covariates (e.g., holidays, time of day) to enhance prediction accuracy.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Data Preparation:**\n",
    "   - Ensure data consistency by handling missing timestamps and filling missing values.\n",
    "   - Add relevant features like holidays, weekdays, and other temporal covariates.\n",
    "\n",
    "2. **Model Training:**\n",
    "   - Train AutoGluon's `TimeSeriesPredictor` using Chronos models.\n",
    "   - Experiment with zero-shot predictions and fine-tuning strategies.\n",
    "\n",
    "3. **Prediction Evaluation:**\n",
    "   - Generate predictions for both training and testing periods.\n",
    "   - Evaluate model performance using various metrics (e.g., RMSE).\n",
    "   - Visualize actual vs predicted traffic metrics to understand model performance.\n",
    "\n",
    "4. **Future Steps:**\n",
    "   - Analyze prediction errors and patterns for further optimization.\n",
    "   - Experiment with different hyperparameters and Chronos models to improve results.\n",
    "\n",
    "### Importance:\n",
    "Accurate traffic predictions enable:\n",
    "- Improved planning for city logistics and resource allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Irrelevant Columns\n",
    "\n",
    "### Objective:\n",
    "To streamline the dataset and focus on features relevant for prediction, we remove constant or redundant columns that do not provide additional information for the model. This helps:\n",
    "- Reduce computational overhead.\n",
    "- Avoid introducing unnecessary noise into the predictive models.\n",
    "\n",
    "### Explanation:\n",
    "1. Columns like `Identifiant arc`, `Libelle`, and node identifiers are constant or categorical identifiers irrelevant for time-series forecasting.\n",
    "2. `Date debut dispo data` and `Date fin dispo data` are mostly missing and do not contribute to the predictive task.\n",
    "\n",
    "### Action:\n",
    "These columns will be dropped to create a clean and optimized dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Libelle', 'Identifiant arc', 'Identifiant noeud amont', 'Libelle noeud amont',\n",
    "                 'Identifiant noeud aval', 'Libelle noeud aval', 'Date debut dispo data', 'Date fin dispo data'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to one-hot encode\n",
    "# columns_to_encode = ['Etat trafic', 'Etat arc', 'nom_vacances', 'nom_jour_ferie', 'DayOfWeek', 'Month']\n",
    "# Apply one-hot encoding to the specified columns\n",
    "# df = pd.get_dummies(df, columns=columns_to_encode, prefix=columns_to_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Time Series Modeling\n",
    "\n",
    "### Objective:\n",
    "To format the dataset into a structure compatible with **AutoGluon's TimeSeriesPredictor**, we need to:\n",
    "1. Add necessary columns for time-series modeling.\n",
    "2. Set up a unique identifier (`item_id`) for the time series.\n",
    "3. Rename columns for standardization.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Add `item_id`:** \n",
    "   - This column uniquely identifies the time series. Since we are working with one series per street, we use a constant identifier (`series_<street>`).\n",
    "\n",
    "2. **Rename Columns:**\n",
    "   - Convert `Date et heure de comptage` to `timestamp` to match AutoGluon requirements.\n",
    "   - Rename the target column (`Débit horaire`) to `target`.\n",
    "\n",
    "3. **Time Conversion:**\n",
    "   - Ensure the `timestamp` column is timezone-naive (necessary for AutoGluon) while retaining the original data.\n",
    "\n",
    "### Importance:\n",
    "These steps ensure compatibility with the **TimeSeriesPredictor** and allow efficient training and prediction on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TimeSeriesDataFrame\n",
    "# df[\"timestamp\"] = df[\"Date et heure de comptage\"]\n",
    "# df.set_index('Date et heure de comptage', inplace=True)\n",
    "\n",
    "# Add the required 'item_id' column (since this is a single time series, we use a constant ID)\n",
    "df['item_id'] = 'series_' + street\n",
    "\n",
    "df[\"Date et heure de comptage\"] = df[\"Date et heure de comptage\"]\n",
    "\n",
    "# Rename \"Date et heure de comptage\" to \"timestamp\"\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "df[\"timestamp\"] = df[\"Date et heure de comptage\"].dt.tz_localize(None)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# Rename \"Débit horaire\" to \"target\"\n",
    "\n",
    "df[\"target\"] = df[target_variable]\n",
    "# df[\"target\"] = df[\"Taux d'occupation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = TimeSeriesDataFrame.from_data_frame(\n",
    "    df,\n",
    "    id_column='item_id',\n",
    ")\n",
    "ts_df.infer_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Recent Data and Missing Values\n",
    "\n",
    "### Objective:\n",
    "To evaluate the completeness of recent data and identify any missing values or inconsistencies in the target variable (`Débit horaire` or `Taux d'occupation`) over a specified timeframe.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Timeframe Definition:**\n",
    "   - Focus on data from November 1, 2024, to December 30, 2024, to capture recent trends and ensure data quality.\n",
    "\n",
    "2. **Visualization:**\n",
    "   - Plot the target variable for the defined period.\n",
    "   - Use individual lines for each `item_id` (if applicable) to highlight missing sections or irregularities.\n",
    "\n",
    "3. **Expected Outcome:**\n",
    "   - Clear visualization of the target variable over time.\n",
    "   - Identification of any gaps or anomalies in the data that may need attention before modeling.\n",
    "\n",
    "### Importance:\n",
    "Analyzing recent data completeness ensures robust model performance and reliable future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define start and end times as pd.Timestamp\n",
    "start_date = pd.Timestamp(\"2024-11-01\")\n",
    "end_date = pd.Timestamp(\"2024-12-30\")\n",
    "\n",
    "# Filter data between start_date and end_date\n",
    "filtered_ts_df = ts_df.slice_by_time(start_time=start_date, end_time=end_date)\n",
    "\n",
    "# Plot the 'target' column\n",
    "for item_id in filtered_ts_df.item_ids:\n",
    "    group = filtered_ts_df.loc[item_id]\n",
    "    plt.plot(group.index, group['target'], label=f\"Item {item_id}\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Target Time Series (July 1st to October 31st)\")\n",
    "# plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))  # Legend outside the plot\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets\n",
    "\n",
    "### Objective:\n",
    "To prepare the data for model training and evaluation by dividing it into a training set and a test set based on a specific cutoff date.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Define Cutoff Date:**\n",
    "   - Set the cutoff date to October 1, 2024. Data before this date will be used for training, and data within the following 10 days will form the test set.\n",
    "   - The goal is to ensure a realistic evaluation of the model's performance on unseen data.\n",
    "\n",
    "2. **Prediction Horizon:**\n",
    "   - Define the prediction length to be 5 days (120 hours). This determines the forecast duration for the model.\n",
    "\n",
    "3. **Data Splitting:**\n",
    "   - Use the cutoff date to divide the `TimeSeriesDataFrame` into:\n",
    "     - `train_data`: Data up to and including the cutoff date.\n",
    "     - `test_data`: Data from the cutoff date until the specified evaluation period (10 days).\n",
    "\n",
    "### Importance:\n",
    "This step ensures that the model is evaluated on a realistic and future-based timeframe, mimicking how predictions will be made in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff point for the test set\n",
    "cutoff_date = df[\"timestamp\"].max() - pd.DateOffset(months=2)\n",
    "# Set cutoff to 25 October 2024\n",
    "cutoff_date = pd.Timestamp(\"2024-10-01\")\n",
    "print(f\"Test set cutoff date: {cutoff_date}\")\n",
    "\n",
    "# Define the prediction length (e.g., 24 hours)\n",
    "prediction_length = 24 * 5\n",
    "\n",
    "# Split the data\n",
    "train_data = ts_df.loc[ts_df.index.get_level_values(\"timestamp\") <= cutoff_date]\n",
    "test_data = ts_df.loc[(ts_df.index.get_level_values(\"timestamp\") > cutoff_date) & (ts_df.index.get_level_values(\"timestamp\") <= cutoff_date + pd.DateOffset(days=10))]\n",
    "\n",
    "# train_data = ts_df.slice_by_time(pd.Timestamp(0), cutoff_date)\n",
    "# test_data = ts_df.slice_by_time(cutoff_date, pd.Timestamp(\"2024-12-31\"))\n",
    "\n",
    "# cutoff_index = ts_df.loc[ts_df.index.get_level_values(\"timestamp\") <= cutoff_date].shape[0]\n",
    "# train_data, test_data = ts_df.train_test_split(end_index=cutoff_index, prediction_length=prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Time Series Model with Chronos and AutoGluon\n",
    "\n",
    "### Objective:\n",
    "To train a **time series prediction model** using AutoGluon's Chronos module. The model aims to forecast traffic metrics (`target`) over a defined horizon (120 hours or 5 days).\n",
    "\n",
    "### Steps:\n",
    "1. **Defining Known Covariates:**\n",
    "   - The following temporal and contextual features are used as covariates to improve prediction accuracy:\n",
    "     - `Year`, `Month`, `Day`, `Hour`, `DayOfWeek`\n",
    "     - `vacances_zone_c`, `ferie` (holidays and bank holidays)\n",
    "\n",
    "2. **Initializing the Predictor:**\n",
    "   - The `TimeSeriesPredictor` is set up with the following key parameters:\n",
    "     - `prediction_length`: The forecast horizon is set to 120 hours (5 days).\n",
    "     - `freq`: Hourly frequency (`'h'`).\n",
    "     - `verbosity`: Logs are set to detailed (`3`).\n",
    "     - `known_covariates_names`: List of known covariates to include in the model.\n",
    "\n",
    "3. **Training Configuration:**\n",
    "   - The Chronos model `amazon/chronos-bolt-mini` is utilized.\n",
    "   - The model is trained in a zero-shot mode for quick predictions without fine-tuning.\n",
    "\n",
    "4. **Training Results:**\n",
    "   - Model training is completed with a validation score of `-0.1006` (WQL metric).\n",
    "   - AutoGluon identifies and excludes non-numeric or non-informative columns (`Date et heure de comptage`, `Etat arc`, etc.).\n",
    "   - The training time is efficient, with a runtime of `0.80 s`.\n",
    "\n",
    "### Importance:\n",
    "By using Chronos and leveraging known covariates, the model can generate more accurate predictions, enabling better traffic management and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "# Specify known covariates\n",
    "known_covariates_names = [\"Year\", \"Month\", \"Day\", \"Hour\", \"DayOfWeek\", \"vacances_zone_c\", \"ferie\"]\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=prediction_length,\n",
    "    verbosity=3,\n",
    "    freq='h',\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    # eval_metric=\"RMSE\"\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_data,\n",
    "    # presets='chronos_mini',\n",
    "    hyperparameters={\n",
    "        # \"Chronos\": {\n",
    "        #     \"use_mps\": True,\n",
    "        #     \"model_path\": \"amazon/chronos-bolt-tiny\",\n",
    "        #     \"fine_tune\": False\n",
    "        # }\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"amazon/chronos-bolt-mini\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            # {\"model_path\": \"amazon/chronos-bolt-tiny\", \"fine_tune\": True, \"ag_args\": {\"name_suffix\": \"FineTuned\"}},\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions with Chronos/AutoGluon\n",
    "\n",
    "### Objective:\n",
    "This step focuses on generating **time-series predictions** for the `target` variable using the trained Chronos model. Predictions utilize known covariates to enhance accuracy.\n",
    "\n",
    "### Steps:\n",
    "1. **Setting the Forecast Horizon:**\n",
    "   - The `get_forecast_horizon_index_ts_dataframe` function creates an index representing the forecast horizon based on the training data and the prediction length (120 hours in this case).\n",
    "\n",
    "2. **Extracting Known Covariates:**\n",
    "   - Known covariates (`Year`, `Month`, `Day`, `Hour`, `DayOfWeek`, `vacances_zone_c`, `ferie`) are extracted from the test dataset for the forecast horizon.\n",
    "   - These covariates are used by the model to improve predictions.\n",
    "\n",
    "3. **Generating Predictions:**\n",
    "   - The `predictor.predict` method is used to generate predictions based on the training data and the known covariates for the forecast horizon.\n",
    "   - Predictions are stored in the `predictions` variable and include probabilistic outputs such as mean and quantile forecasts.\n",
    "\n",
    "### Importance:\n",
    "By incorporating covariates, the predictions become more context-aware, potentially leading to higher accuracy and reliability for traffic forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries.utils.forecast import get_forecast_horizon_index_ts_dataframe\n",
    "\n",
    "# Generate predictions\n",
    "# Without covariates\n",
    "# predictions = predictor.predict(train_data)\n",
    "\n",
    "# With covariates\n",
    "future_index = get_forecast_horizon_index_ts_dataframe(train_data, prediction_length=prediction_length)\n",
    "future_timestamps = future_index.get_level_values(\"timestamp\")\n",
    "known_covariates = test_data[known_covariates_names]  # Known covariates from test data\n",
    "predictions = predictor.predict(train_data, known_covariates=known_covariates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Time-Series Predictor\n",
    "\n",
    "### Objective:\n",
    "This section evaluates the performance of the trained Chronos model using test data and key metrics such as Weighted Quantile Loss (WQL) and Root Mean Square Error (RMSE).\n",
    "\n",
    "### Steps:\n",
    "1. **Preparing the Evaluation Dataset:**\n",
    "   - The evaluation dataset (`eval_data`) combines:\n",
    "     - The original training data.\n",
    "     - The first `prediction_length` rows (e.g., 120 hours) of the test data for evaluation.\n",
    "\n",
    "2. **Running the Evaluation:**\n",
    "   - The `predictor.evaluate` method computes the model's performance on the evaluation dataset using:\n",
    "     - **WQL**: A weighted quantile loss that considers probabilistic predictions.\n",
    "     - **RMSE**: A standard metric that measures the average prediction error in terms of square differences.\n",
    "\n",
    "3. **Output Metrics:**\n",
    "   - **WQL**: -0.0907\n",
    "   - **RMSE**: -129.69\n",
    "\n",
    "### Importance:\n",
    "These metrics provide insight into the model's accuracy and robustness. A low WQL and RMSE indicate that the predictions closely align with actual values, making the model reliable for practical traffic forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the evaluation dataset\n",
    "# Combine train_data and the first prediction_length rows of test_data\n",
    "eval_data = TimeSeriesDataFrame(pd.concat(\n",
    "    [\n",
    "        train_data,\n",
    "        test_data.groupby(\"item_id\").head(prediction_length)  # First prediction_length rows of test_data\n",
    "    ]\n",
    "))\n",
    "\n",
    "# Evaluate the predictor\n",
    "performance = predictor.evaluate(eval_data, metrics=[\"WQL\", \"RMSE\"])\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Evaluation Performance Metrics:\")\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cutoff_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions vs. Actual Values\n",
    "\n",
    "### Objective:\n",
    "This visualization compares the **actual values** of the traffic metric (`target`) with the **predicted values** generated by the Chronos model. The purpose is to assess how closely the model's predictions align with observed data over time.\n",
    "\n",
    "### Key Features:\n",
    "1. **Actual Values:**\n",
    "   - Represented by a solid blue line.\n",
    "   - Reflects the real observed data during the test period.\n",
    "\n",
    "2. **Predicted Values:**\n",
    "   - Represented by a dashed orange line.\n",
    "   - Shows the forecast generated by the trained model for the same time period.\n",
    "\n",
    "3. **Key Insights:**\n",
    "   - Overlapping areas between the two lines indicate accurate predictions.\n",
    "   - Discrepancies highlight areas where the model's performance can be improved.\n",
    "\n",
    "### Importance:\n",
    "This plot provides a clear visual evaluation of the model's accuracy and helps identify patterns, anomalies, or trends in the forecasting results. Such insights are crucial for refining the model and optimizing its performance for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure predictions and test data have the same structure\n",
    "# Assuming 'target' is the column in test_data and predictions\n",
    "\n",
    "# Plot predictions vs. actual values\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(test_data.index.get_level_values('timestamp'),\n",
    "         test_data['target'],\n",
    "         label='Actual Values',\n",
    "         linestyle='-')\n",
    "\n",
    "# Plot predicted values\n",
    "plt.plot(predictions.index.get_level_values('timestamp'),\n",
    "         predictions['mean'],\n",
    "         label='Predicted Values',\n",
    "         linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Target Value')\n",
    "plt.title('Predictions vs. Actual Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Align the MultiIndex between predictions and test data\n",
    "common_index = predictions.index.intersection(test_data.index)\n",
    "\n",
    "# Extract aligned actual values and predictions\n",
    "aligned_actuals = test_data.loc[common_index, \"target\"]\n",
    "aligned_predictions = predictions.loc[common_index, \"mean\"]\n",
    "\n",
    "# Drop NaN values\n",
    "valid_mask = ~aligned_actuals.isna() & ~aligned_predictions.isna()\n",
    "aligned_actuals = aligned_actuals[valid_mask]\n",
    "aligned_predictions = aligned_predictions[valid_mask]\n",
    "\n",
    "# Ensure indices are aligned for plotting\n",
    "aligned_actuals = aligned_actuals.sort_index()\n",
    "aligned_predictions = aligned_predictions.sort_index()\n",
    "\n",
    "# Calculate RMSE for train_data predictions\n",
    "rmse_train = np.sqrt(mean_squared_error(aligned_actuals, aligned_predictions))\n",
    "print(f\"RMSE (Next 5 days after train): {rmse_train}\")\n",
    "\n",
    "# Evaluate RMSE with extended train_data + partial test_data\n",
    "durations = [24, 48, 72, 96]  # Example durations in hours\n",
    "rmse_list = []\n",
    "\n",
    "for duration in durations:\n",
    "    # Extend train_data with the first 'duration' hours of test_data\n",
    "    extended_train_data = pd.concat([\n",
    "        train_data,\n",
    "        test_data.iloc[:duration]\n",
    "    ])\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions_extended = predictor.predict(extended_train_data, known_covariates=known_covariates)\n",
    "    \n",
    "    # Align predictions and actual values\n",
    "    extended_common_index = predictions_extended.index.intersection(test_data.index)\n",
    "    aligned_actuals = test_data.loc[extended_common_index, \"target\"]\n",
    "    aligned_predictions = predictions_extended.loc[extended_common_index, \"mean\"]\n",
    "    \n",
    "    # Drop NaN values\n",
    "    valid_mask = ~aligned_actuals.isna() & ~aligned_predictions.isna()\n",
    "    aligned_actuals = aligned_actuals[valid_mask]\n",
    "    aligned_predictions = aligned_predictions[valid_mask]\n",
    "    \n",
    "    # Ensure indices are aligned for RMSE calculation\n",
    "    aligned_actuals = aligned_actuals.sort_index()\n",
    "    aligned_predictions = aligned_predictions.sort_index()\n",
    "    \n",
    "    # Calculate RMSE for the current duration\n",
    "    rmse = np.sqrt(mean_squared_error(aligned_actuals, aligned_predictions))\n",
    "    rmse_list.append(rmse)\n",
    "    print(f\"RMSE after adding {duration} hours of test data: {rmse}\")\n",
    "\n",
    "# Calculate average RMSE over all durations\n",
    "average_rmse = np.mean(rmse_list)\n",
    "print(f\"Average RMSE: {average_rmse}\")\n",
    "\n",
    "# Plot predictions vs actual values for train_data\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(\n",
    "    aligned_actuals.index.get_level_values('timestamp'),\n",
    "    aligned_actuals.values,\n",
    "    label='Actual Values',\n",
    "    linestyle='-'\n",
    ")\n",
    "\n",
    "# Plot predicted values for train_data\n",
    "plt.plot(\n",
    "    aligned_predictions.index.get_level_values('timestamp'),\n",
    "    aligned_predictions.values,\n",
    "    label='Predicted Values (Train Data)',\n",
    "    linestyle='--'\n",
    ")\n",
    "\n",
    "# Optionally, add extended train_data predictions to the plot\n",
    "for duration in durations:\n",
    "    extended_train_data = pd.concat([train_data, test_data.iloc[:duration]])\n",
    "    predictions_extended = predictor.predict(extended_train_data, known_covariates=known_covariates)\n",
    "    extended_common_index = predictions_extended.index.intersection(test_data.index)\n",
    "    aligned_predictions = predictions_extended.loc[extended_common_index, \"mean\"]\n",
    "    \n",
    "    # Ensure indices are aligned for plotting\n",
    "    aligned_predictions = aligned_predictions.sort_index()\n",
    "    \n",
    "    plt.plot(\n",
    "        aligned_predictions.index.get_level_values('timestamp'),\n",
    "        aligned_predictions.values,\n",
    "        linestyle=':',\n",
    "        label=f'Predicted (Train + {duration}h Test)'\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Target Value')\n",
    "plt.title('Predictions vs. Actual Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = TimeSeriesDataFrame.from_data_frame(\n",
    "    df,\n",
    "    id_column='item_id',\n",
    ")\n",
    "ts_df.infer_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff date to 6 December 2024 00:30:00 Paris time\n",
    "cutoff_date = pd.Timestamp(\"2024-12-06 00:30:00\").tz_localize(\"Europe/Paris\").tz_convert(\"UTC\").tz_localize(None)\n",
    "print(f\"Predictions cutoff date: {cutoff_date.tz_localize('UTC').tz_convert('Europe/Paris')}\")\n",
    "\n",
    "# Define the prediction length (e.g., 24 hours)\n",
    "prediction_length = 24 * 5\n",
    "\n",
    "# Split the data\n",
    "train_data = ts_df.loc[ts_df.index.get_level_values(\"timestamp\") <= cutoff_date]\n",
    "test_data = ts_df.loc[(ts_df.index.get_level_values(\"timestamp\") > cutoff_date)]\n",
    "\n",
    "# Print last date of train_data\n",
    "print(f\"Last date of train_data: {train_data.index.get_level_values('timestamp').max().tz_localize('UTC').tz_convert('Europe/Paris')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the predictor without fine-tuning\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=prediction_length,\n",
    "    verbosity=3,\n",
    "    freq='h',\n",
    "    known_covariates_names=known_covariates_names,\n",
    "    # eval_metric=\"RMSE\"\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_data,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"amazon/chronos-bolt-mini\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            # {\"model_path\": \"amazon/chronos-bolt-tiny\", \"fine_tune\": True, \"ag_args\": {\"name_suffix\": \"FineTuned\"}},\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries.utils.forecast import get_forecast_horizon_index_ts_dataframe\n",
    "\n",
    "# Generate predictions\n",
    "# Without covariates\n",
    "# predictions = predictor.predict(train_data)\n",
    "\n",
    "# With covariates\n",
    "future_index = get_forecast_horizon_index_ts_dataframe(train_data, prediction_length=prediction_length)\n",
    "future_timestamps = future_index.get_level_values(\"timestamp\")\n",
    "known_covariates = test_data[known_covariates_names]  # Known covariates from test data\n",
    "predictions = predictor.predict(train_data, known_covariates=known_covariates)\n",
    "\n",
    "\n",
    "print(\"First prediction date:\", predictions.index.get_level_values(\"timestamp\").min().tz_localize(\"UTC\").tz_convert(\"Europe/Paris\"))\n",
    "print(\"Last prediction date:\", predictions.index.get_level_values(\"timestamp\").max().tz_localize(\"UTC\").tz_convert(\"Europe/Paris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot predictions vs. actual values\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot predicted values\n",
    "plt.plot(predictions.index.get_level_values('timestamp'),\n",
    "         predictions['mean'],\n",
    "         label='Predicted Values',\n",
    "         linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Target Value')\n",
    "plt.title('Predictions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "output_dir = \"predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert the TimeSeriesDataFrame to a pandas DataFrame\n",
    "predictions_df = predictions.to_data_frame()\n",
    "\n",
    "# Iterate over each time series in the DataFrame\n",
    "for street, street_data in predictions_df.groupby(level=0):  # Assumes street info is in the first level of the index\n",
    "    # Save to a CSV file\n",
    "    output_file = f\"{output_dir}/{street}-{target_variable}.csv\"\n",
    "    street_data.reset_index(level=0, drop=True).to_csv(output_file)\n",
    "\n",
    "print(f\"Predictions saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
